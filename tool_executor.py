from dotenv import load_dotenv, find_dotenv

load_dotenv(find_dotenv(), override=True)

from typing import List

from langchain_core.messages import BaseMessage, ToolMessage, HumanMessage, AIMessage

from schemas import AnswerQuestion, Reflection

from langgraph.prebuilt import ToolInvocation, ToolExecutor

from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper
from langchain_community.tools.tavily_search import TavilySearchResults

from collections import defaultdict

import json

from chains import json_parser

search = TavilySearchAPIWrapper()
tavily_tool = TavilySearchResults(api_wrapper=search, max_results=5)
tool_executor = ToolExecutor(
    [tavily_tool]
)  # used to invocke multiple tavily searches asynchronously/in parallel (at the same time, because there is a batch method)


def execute_tools(
    state: List[BaseMessage],
) -> List[ToolMessage]:  # function that our "execute tools" node is going to run
    tool_invocation: AIMessage = state[-1]  # the last state/last message
    parsed_tool_calls = json_parser.invoke(tool_invocation)

    ids = []
    tool_invocations = []

    for parsed_call in parsed_tool_calls:
        for query in parsed_call["args"]["search_queries"]:
            # print(query)
            tool_invocations.append(
                ToolInvocation(tool="tavily_search_results_json", tool_input=query)
            )
            ids.append(parsed_call["id"])
    outputs = tool_executor.batch(inputs=tool_invocations)

    # Map each output to its corresponding ID and tool input
    outputs_map = defaultdict(dict)
    for id_, output, invocation in zip(ids, outputs, tool_invocations):
        outputs_map[id_][invocation.tool_input] = output
        # Example structure of this data: outputs_map = {id: {search_query_1: [search_result_1, search_result_2, search_result_3], search_query_2: [search_result_4, search_result_5, search_result_6]}}

    # Convert the mapped outputs to ToolMessage objects
    tool_messages = []
    for id_, mapped_output in outputs_map.items():
        tool_messages.append(
            ToolMessage(content=json.dumps(mapped_output), tool_call_id=id_)
        )

    return tool_messages  # This will be appended to GraphState


if __name__ == "__main__":
    print("Tool Executor Enter")

    human_message = HumanMessage(
        content="Write about AI-Powered SOC / autonomous soc  problem domain,"
        " list startups that do that and raised capital."
    )

    answer = AnswerQuestion(
        answer="",
        reflection=Reflection(missing="", superfluous=""),
        search_queries=[
            "AI-powered SOC startups funding",
            "AI SOC problem domain specifics",
            "Technologies used by AI-powered SOC startups",
        ],
        id="call_KpYHichFFEmLitHFvFhKy1Ra",
    )  # id is auto-generated by the llm vendor (OpenAI)
    # this is an example state (as in, execute_tools(state))

    raw_response = execute_tools(
        state=[
            human_message,
            AIMessage(
                content="",
                tools_calls=[
                    {
                        "name": AnswerQuestion.__name__,
                        "args": answer.dict(),
                        "id": "call_KpYHichFFEmLitHFvFhKy1Ra",
                    }
                ],
            ),
        ]
    )  # using an example state

    print(raw_response)
